"""
Module Description
==================

This module provides ECT's data access API.

Module Requirements
===================

**Query catalogue**

:Description: Allow querying registered ECV catalogues using a simple function that takes a set of query parameters
    and returns data source identifiers that can be used to open respective ECV dataset in the ECT.
:Specified in: <link to other RST page here>
:Test: <link to test class.function here>
:URD-Source: <name the URD # and optionally the name>

----

**Open dataset**

:Description: Allow opening an ECV dataset given an identifier returned by the *catalogue query*.
   The dataset returned complies to the ECT common data model.
   The dataset to be returned can optionally be constrained in time and space.
:Specified in: <link to other RST page here>
:Test: <link to test class.function here>
:URD-Source: <name the URD # and optioanlly the name>



Module Reference
================
"""
from typing import Sequence, Union, List
from ect.core import Dataset
import json
from datetime import date, datetime, timedelta
import xarray as xr

from ect.core.cdm_xarray import XArrayDatasetAdapter


class DataSource:
    def __init__(self, name: str):
        self._name = name

    @property
    def name(self) -> str:
        return self._name

    def open_dataset(self, **constraints) -> Dataset:
        return None

    def matches_filter(self, **filter) -> bool:
        if filter:
            for key, value in filter.items():
                if key == 'name' and not value in self._name:
                    return False
        return True


class Catalogue:
    def __init__(self, *data_sources: DataSource):
        self._data_sources = data_sources

    def query(self, **filter) -> [DataSource]:
        return [ds for ds in self._data_sources if ds.matches_filter(**filter)]


# DEFAULT_CATALOGUE = Catalogue(DataSource("default", "default"))


def query_data_sources(catalogues: Union[Catalogue, Sequence[Catalogue]] = None, **constraints) -> List[
    DataSource]:
    """Queries the catalogue(s) for data sources matching the given constrains.

    Parameters
    ----------
    catalogues : Catalogue or Sequence[Catalogue]
       If given these catalogues will be querien. Othewise the DEFAULT_CATALOGUE will be used
    constraints : dict, optional
       The contains may limit the dataset in space or time.

    Returns
    -------
    datasource : List[DataSource]
       All data sources matching the given constrains.

    See Also
    --------
    open_dataset
    """

    if isinstance(catalogues, Catalogue):
        catalogue_list = [catalogues]
    else:
        catalogue_list = catalogues
    results = []
    for catalogue in catalogue_list:
        results.extend(catalogue.query(**constraints))
    return results


def open_dataset(data_source: Union[DataSource, str], **constraints) -> Dataset:
    """Load and decode a dataset.

    Parameters
    ----------
    data_source : str or DataSource
       Strings are interpreted as the identifier of an ECV dataset.
    constraints : str, optional
       The contains may limit the dataset in space or time.

    Returns
    -------
    dataset : Dataset
       The newly created dataset.

    See Also
    --------
    query_data_sources
    """
    if data_source is None:
        raise ValueError('No data_source given')

    if isinstance(data_source, str):
        raise NotImplementedError # TODO
        # data_source = query_data_sources(DEFAULT_CATALOGUE, name=data_source)
    return data_source.open_dataset(**constraints)


#########################

def _as_datetime(dt: Union[str, datetime], default) -> datetime:
    if dt is None:
        return default
    if isinstance(dt, str):
        # TODO handle format with/without time
        return datetime.strptime(dt, "%Y-%m-%d")
    if isinstance(dt, datetime):
        return dt
    raise ValueError


class FileSetDataSource(DataSource):
    """A class representing the a specific file set with the meta information belonging to it.

    Parameters
    ----------
    name : str
        The name of the file set
    base_dir : str
        The base directory
    file_pattern : str
        The file pattern with wildcards for year, month, and day
    fileset_info : FileSetInfo
        The file set info generated by a scannning, can be None

    Returns
    -------
    new  : FileSetDataSource
    """

    def __init__(self, name: str, base_dir: str, file_pattern: str, fileset_info: 'FileSetInfo' = None):
        super(FileSetDataSource, self).__init__(name)
        self._name = name
        self._base_dir = base_dir
        self._file_pattern = file_pattern
        self._fileset_info = fileset_info

    def open_dataset(self, **constraints) -> Dataset:
        first_time = constraints.get('first_time', None)
        last_time = constraints.get('last_time', None)
        paths = self._resolve_paths(first_time=first_time, last_time=last_time)
        xr_dataset = xr.open_mfdataset(paths)
        cdm_dataset = XArrayDatasetAdapter(xr_dataset)
        return cdm_dataset

    @property
    def _full_pattern(self) -> str:
        return self._base_dir + "/" + self._file_pattern

    def _resolve_paths(self, first_time: Union[str, datetime] = None, last_time: Union[str, datetime] = None) -> Sequence[str]:
        """Return a list of all paths between the given times.

        For all dates, including the first and the last time, the wildcard in the pattern is resolved for the date.

        Parameters
        ----------
        first_time : str or datetime
            The first date of the time range, can be None if the file set has a *start_time*.
            In this case the *start_time* is used.
        last_date : str or datetime
            The last date of the time range, can be None if the file set has a *end_time*.
            In this case the *end_time* is used.
        """
        if first_time is None and self._fileset_info._start_time is None:
            raise ValueError("neither first_time nor start_time are given")
        dt1 = _as_datetime(first_time, self._fileset_info._start_time)

        if last_time is None and self._fileset_info._end_time is None:
            raise ValueError("neither last_time nor end_time are given")
        dt2 = _as_datetime(last_time, self._fileset_info._end_time)

        if dt1 > dt2:
            raise ValueError("start time '%s' is after end time '%s'" % (dt1, dt2))

        return [self._resolve(dt1 + timedelta(days=x)) for x in range((dt2 - dt1).days + 1)]

    def _resolve(self, date: date):
        path = self._full_pattern
        if "{YYYY}" in path:
            path = path.replace("{YYYY}", "%04d" % (date.year))
        if "{MM}" in path:
            path = path.replace("{MM}", "%02d" % (date.month))
        if "{DD}" in path:
            path = path.replace("{DD}", "%02d" % (date.day))
        return path


class FileSetInfo:
    def __init__(self,
                 info_update_time: Union[str, datetime],
                 start_time: Union[str, datetime],
                 end_time: Union[str, datetime],
                 num_files: int,
                 size_in_mb: int):
        self._start_time = _as_datetime(start_time, None)
        self._end_time = _as_datetime(end_time, None)
        self._num_files = num_files
        self._size_in_mb = size_in_mb


class FileSetCatalogue(Catalogue):
    def __init__(self, root_dir: str, fileset_datasources: Sequence[FileSetDataSource]):
        super(FileSetCatalogue, self).__init__(*fileset_datasources)
        self._root_dir = root_dir

    @property
    def root_dir(self) -> str:
        return self._root_dir


def fileset_datasources_from_json(json_str) -> Sequence[FileSetDataSource]:
    fsds = []
    for data in json.loads(json_str):
        fsds.append(FileSetDataSource(
            data['name'],
            data['base_dir'],
            data['file_pattern'],
            FileSetInfo(
                datetime.now(),  # TODO
                data['start_date'],
                data['end_date'],
                data['num_files'],
                data['size_mb']
            )
        ))
    return fsds


def fileset_catatalogue_from_file(filename: str, root_dir: str) -> FileSetCatalogue:
    with open(filename) as json_file:
        json = json_file.read()
    fileset_datasources = fileset_datasources_from_json(json)
    return FileSetCatalogue(root_dir, fileset_datasources)
